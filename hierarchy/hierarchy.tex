% \documentclass[11pt,a4paper]{article}
% \documentclass[man,floatsintext]{apa6}
\documentclass[doc]{apa6}

% \usepackage[margin=2.5cm]{geometry}

\usepackage{graphicx}
\usepackage{subfig}
% \usepackage{subcaption}

\usepackage{amsmath}
\usepackage{amsfonts}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\usepackage[american]{babel}
\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}
\DeclareLanguageMapping{american}{american-apa}
\addbibresource{library.bib}


\title{The cluster representation of knowledge can appear as if knowledge is hierarchically
structured.}

\shorttitle{Structure Optimization}
\author{Takao Noguchi, Brad Love}
\affiliation{Department of Experimental Psychology, University College London}

\abstract{%

    Environments surrounding people are often hierarchically structured: a single object usually
    fits into a series of progressively more general categories (e.g., a terrier, dog, and animal).
    Given the prevalence of this hierarchical structure, previous studies often assumed that
    knowledge representation is also hierarchical, although empirical evidence does not provide a
    clear support. In this contribution, we demonstrate that neither the environment nor knowledge
    representations need to be hierarchically structured for people to prefer the hierarchical
    structure: the hierarchical structure enables people to make the most accurate inference and
    should be preferred, even when knowledge representation is flat. This optimality is most
    pronounced when knowledge is represented with clusters of observed objects.  Further, the
    cluster representation naturally leads to the effects of learning order, which explain
    mechanisms behind the basic level advantage and the age of acquisition effect.

}


\authornote{%

    Word count: 2276 (excluding the appendix).

    Last updated: \today.

}


\begin{document}

\maketitle

Even informal observation of everyday categorization reveals that many objects fit into a number of
categories. A single object might be called a terrier, dog, mammal or animal. This hierarchical
structure of categories --- a sequence of progressively larger categories --- has been suggested as
a universal property of category structure across cultures \parencite{Berlin1992a, Atran1998a}.
Given the wide-spread adaptation of the hierarchical structure, it is tempting to assume that
knowledge is hierarchically represented. This assumption of hierarchical representation, indeed,
often underlies theoretical propositions \parencite[e.g.,][]{Tenenbaum2011a}. In this study,
however, we demonstrate that the hierarchical structure should be preferred even when knowledge
representation is flat, without the depth represented in the hierarchical structure.

% *** again, i think there is an idea out there that world might be hierarchical. there might be a
% nice parallel to work in prototype theory. rosch shows people prefer prototypes, so we assume
% prototype model of concepts, then comes along exemplar models. Likewise, Rosch and others say
% hierarchy, so people assume some hierarchal organisation, but again that's not necessary.


\subsection*{Hierarchical structure of categories}

\begin{figure}
    \centering

    \subfloat[Hierarchical structure, where dogs share the maximum number of feature values at both
    levels, and categories at the specific level are nested within categories at the general level.] {%

        \includegraphics[]{figure/example_hierarchy.pdf}
    }

    \subfloat[Non-hierarchical structure, where dogs share the maximum number of feature values at
    each level, but categories at the specific level are orthogonal to categories at the general level.]
    {%

        \includegraphics[]{figure/example_nonhierarchy.pdf}
    }

    \vspace{10pt}

    \caption{Example structures with dogs. A rectangle encloses dogs in a category, with a thicker
    rectangle indicating a category at the general level.}

\label{fig:example}
\end{figure}

To begin, let us illustrate the hierarchical structure.  An example hierarchical structure is
illustrated in Figure~\ref{fig:example} (a).  For brevity, we assume discrete features. Then
formally, the hierarchical structure satisfies the two characteristics: maximization of feature
information and the inclusion relation.  First, the hierarchical structure maximizes the information
which objects, $X$, provide about a category $y$:
\begin{align}
    I \left( y;\, X \right) = H \left( X \right) - H \left( X \middle\vert\, y \right),
\label{eqn:feature}
\end{align}
which is mutual information (denoted as $I$) between category labels and objects. This feature
information is maximum when the conditional entropy, denoted as $H \left( X \middle\vert\, y
\right)$, is minimum. This is achieved when all the objects in a category share values on the
largest possible number of features.

The hierarchical structure also satisfies the inclusion relation, that all the objects in one
category at the specific level fit into the same category at the general level (see
Figure~\ref{fig:example} for examples).

Previous studies have demonstrated preference for the hierarchical structure.  When asked
to categorize objects in any manner they prefer, for example, people are very likely to provide
hierarchical structure of categories \parencite[e.g.,][]{Rosch1976a}. When categorizing the eight
dogs in Figure~\ref{fig:example}, for example, people are more likely to produce a structure similar
to Figure~\ref{fig:example} (a) than (b).

Given the wide-spread preference for hierarchical structure of categories, it has been argued that
knowledge is hierarchically represented \parencite[e.g.,][]{Markman1984a, Markman1989a}.
\textcite{Markman1989a}, for example, argues that people learn to hierarchically represent knowledge
as they accumulate knowledge \parencite[see also][]{Vygotsky1962a, Inhelder1964a}.

This assumption of hierarchical representation has been underlying theoretical propositions in
cognitive science. For example, a mathematical model has been proposed to explain how people could
learn to hierarchically represent knowledge \parencite{Kemp2008a}, and an inference drawn with this
model correlates with inference people make \parencite{Kemp2009a}.  The assumption of hierarchical
representation also underlies theories in many other domains within cognitive science: for example,
inference \parencite{Osherson1990a}, memory \parencite{Bower1969a, Glass1975a}, reasoning
\parencite{Collins1989a, Shastri1993a}, and word learning \parencite{Xu2007a}.

The assumption of hierarchical representation, however, is not well supported with empirical
evidence \parencite[see][for review]{Murphy1997a}. \textcite{Sloman1998a}, for example, report that
when reasoning and making inferences with the hierarchical structure, people often neglect the
inclusion relation. Thus despite the preference for the hierarchical structure, an inference people
make does not appear to follow from the hierarchical knowledge representation.

In this study, we step back and consider the possibility that preference for hierarchy is not an
indication of hierarchical representation.  In particular, we show that even when models of category
learning do not assume the hierarchical structure, the hierarchical structure should still be
preferred.  Then, we demonstrate that these models also predict how early learning shapes knowledge
representation and influences inference accuracy later in life.


\subsection*{Models of human cognition}

Unlike the theoretical propositions discussed above, many cognitive models of category learning do
not assume the hierarchical representation. Here, we discuss two of the dominant representations:
cluster and exemplar representations. The cluster representation assumes that people represent
knowledge as clusters of the observed objects \parencite[e.g.,][]{Anderson1991a, Love2004a,
Sanborn2010a}.

Every time a new object is observed, the new object is assigned to an existing cluster with similar
objects (see Figure~\ref{fig:cluster} for an example). If none of the existing clusters contains
sufficiently similar objects, a new cluster is created to represent the new object.  Then using the
knowledge represented as clusters, an inference is drawn from the observed objects in the cluster.
If a new object is assigned to a cluster predominantly with terriers, for example, the new object is
considered to be a terrier.

\begin{figure}
    \centering

    \includegraphics[]{figure/example_cluster.pdf}
    \vspace{6pt}

    \caption{Example clusters. An ellipse encloses dogs in a cluster. A new observation is assigned
    to a cluster which contains most similar dogs.}

\label{fig:cluster}
\end{figure}

Under the hierarchical structure as discussed above, the feature information is at the maximum, and
objects in the same category share values on as many features as possible.  Since objects which
share values tend to be assigned to the same cluster, the cluster structure mirrors the category
structure at one level.  The cluster structure illustrated in Figure~\ref{fig:cluster}, for example,
mirrors the category structure at the specific level in Figure~\ref{fig:example} (a): each cluster
contains dogs with the same ear (pointy or floppy) and of the same size (small or large).  Then as
the objects in the same cluster fit into the same category, an inference on the category labels
tends to be more accurate.

In contrast, a non-hierarchical category structure cannot be mirrored in the cluster structure.
With the structure in Figure~\ref{fig:example} (b), for example, if the cluster structure mirrors
the category structure at the specific level, objects in the same cluster fit into different
categories at the general level, and then, an inference cannot be accurate at the general level.
The only way to allow an accurate inference with the non-hierarchical category structure is to
assign only one object to each cluster, which is often termed the exemplar representation
\parencite{Nosofsky1986a, Nosofsky1991a} (see Figure~\ref{fig:exemplar} for an illustration).

\begin{figure}
    \centering

    \includegraphics[]{figure/example_exemplar.pdf}
    \vspace{6pt}

    \caption{Exemplar representation, where each object is assigned to its own cluster.}

\label{fig:exemplar}
\end{figure}

With the exemplar representation, each cluster contains a single object, and an inference is based
on one of the most similar among the observed objects.  If an object is most similar to an observed
object labeled as a terrier, for example, the object is inferred to be a terrier.  Thus, an
inference is more accurate when similar objects fit into the same category, which corresponds to
when the feature information is maximized.  As the cluster structure cannot mirror the category
structure, however, the exemplar representation is insensitive to the inclusion relation: whether
objects in a category at the specific level fit into the same category at the general
level\footnote{We assume that people do not infer a category label at another level than being
asked: when making an inference on the general category, people do not infer the specific category
\parencite[see also,][]{Nosofsky2015a}.}\footnote{The insensitivity to the inclusion relation
reflects people's neglect of the inclusion relation in making an inference, as discussed above. The
insensitivity to the inclusion relation, however, can stem from the cluster representation, which we
discuss in relation to influences of learning orders later in this article.}.

Therefore, the hierarchical structure is expected to be optimal, such that a human learner is most
likely to make an accurate inference across multiple levels of categories. This optimality, however,
depends on knowledge representation. With the cluster representation, both the feature information
and the inclusion relation improve the inference accuracy. With the exemplar representation,
however, only the feature information improves the inference accuracy. To demonstrate this, we ran a
simulation study reported in the next section.


\subsection*{Preference for the hierarchical structure}

\begin{figure}[t!]
    \centering

    \subfloat[Cluster representation. An inference is most accurate at both levels, when the feature
    information is maximum and the inclusion relation is satisfied.]
    {%
        \includegraphics[]{./figure/hierarchy_accuracy_cluster.pdf}
    }

    \subfloat[Exemplar representation. An inference is more accurate when the feature information is
    maximum but the accuracy is not affected by the inclusion relation.]
    {%
        \includegraphics[]{./figure/hierarchy_accuracy_exemplar.pdf}
    }

    \caption{The average inference accuracy for each category structure. The levels of categories
    are shown in columns: the left panels illustrate the general level, and the right panels
    illustrate the specific level.  A dot represents mean, and error bar is the empirical interval.
    Red summarizes hierarchical structures, and black summarizes non-hierarchical structures.}

\label{fig:result}
\end{figure}

We took the eight dogs with the three features in Figure~\ref{fig:example} and tested all the
possible category structures. For brevity, we constrained the category structures, such that the
general level has two categories with four dogs each, and the specific level contains four
categories with two dogs each.

For each category structure, the model is trained for 10 blocks. Each block involves 16 trials, the
eight objects with a category label at either general or specific level, in a random order. After
the training blocks, the average inference accuracy was calculated for each level of categories.
This accuracy was further mean-averaged across the $10^{4}$ simulations for each category structure
and summarized in Figure~\ref{fig:result} (Please refer to Appendix~\ref{appendix:model} for more
details of the models and the simulation).

Figure~\ref{fig:result} shows that with the cluster representation, the inference is more accurate
with the feature information and the inclusion relation: The highest accuracy is achieved when the
feature information is maximum and the inclusion relation is satisfied.  With the exemplar
representation, in contrast, the inference is more accurate only with the feature information.
Here, the highest accuracy is achieved with the maximum feature information but the inclusion
relation does not appear to have an impact.

Thus, the simulation results show that the hierarchical structure is optimal given human cognition.
This optimality is more pronounced with the cluster representation than with the exemplar
representation.


% This is because the identical objects are repeatedly presented in the simulation, and hence, an
% inference is often based on the previously-presented identical object, as opposed to the most
% similar object among the observed ones.  Thus, the similarity between objects tend to have little
% impact on inference.

% While the cluster representation shows improved inference accuracy with both feature information
% and inclusion relation, the exemplar representation shows improved inference accuracy only with
% the feature information.


\subsection*{Advantage of certain level in the hierarchical structure}

The above simulation proves that knowledge does not have to be hierarchically represented for the
hierarchical structure to be preferred. This demonstration may appear in contradiction to previous
findings which concern the hierarchical representation: in particular, the basic level advantage
\parencite{Mervis1981a, Rosch1976a} and the age of acquisition effects \parencite{Gerhand1998a,
Morrison1995a}.

The basic level advantage documents that an inference people make is generally more accurate and
faster at the basic level (e.g., dog) than at a more general (e.g., mammal) or more specific level
(e.g., terrier). This basic level advantage has been explained with the assumption that knowledge is
hierarchically represented.

\textcite{Jolicoeur1984a}, for example, argue that knowledge is accessed at the basic level at first
and knowledge at other levels is subsequently accessed through the basic level. This proposition is
not well supported by empirical studies. Clinical patients with semantic dementia, for example, can
make an inference at the general level but cannot at the basic level \parencite{Hodges1995a},
suggesting that the patients can access to knowledge at the general level without accessing
knowledge at the basic level. To resolve this contradiction, \textcite{Rogers2007a} argue that the
basic level advantage can be observed even when knowledge at the general level is accessed at first.

Here again, we take a step back and consider the possibility that knowledge is not hierarchically
represented. In this vein, key findings are from developmental studies, which report that categories
at the basic level are learned earlier in the childhood \parencite{Berlin1973a, Brown1958a,
Horton1980a, Mervis1982a}.  Thus, the basic level advantage appears associated with learning order:
an inference tends to be more accurate at a category level learned at first.  Similarly, the age of
acquisition effect documents that words which are acquired earlier in childhood are processed more
accurately than words that are acquired later in life.

\begin{figure}
    \centering

    \subfloat[When categories at the general level are learned first, the cluster structure mimics
    the category structure at the general level, allowing an accurate inference at the general level.]
    {%

        \includegraphics[]{figure/example_higher_first.pdf}
    }

    \subfloat[When categories at the specific level are learned first, the cluster structure mimics
    the category structure at the specific level, allowing an accurate inference at the specific level.]
    {%

        \includegraphics[]{figure/example_lower_first.pdf}
    }

    \vspace{10pt}

    \caption{Illustrative cluster structures for each learning order. An ellipse encloses dogs in a
    cluster.}

\label{fig:learning_order}
\end{figure}

These effects of learning order are naturally borne out of the cluster representation.
Figure~\ref{fig:learning_order} (a) illustrates a typical knowledge representation after learning
the categories at the general level. As the cluster assignment of an object depends on its category
label as well as its features, objects from the same category tend to be perceived similar and fit
into the same cluster.  As a result, the cluster structure tends to mirror the category structure
learned at first.

Subsequent learning at another category level builds upon this initial cluster structure, and hence,
the cluster structure cannot follow as closely the category structure learned later.  Consequently,
an inference tends to be more accurate at the category level learned at first.  With the knowledge
represented as in Figure~\ref{fig:learning_order} (a), for example, an inference at the general
level may be accurate, but an inference at the specific level may not be as accurate. This is
because each cluster contains dogs from multiple categories at the specific level.

When a cluster contains objects from multiple categories, an inference drawn with the cluster can
appear to neglect the inclusion relation \parencite[e.g.,][]{Sloman1998a}.  When knowledge about the
dogs is represented as in Figure~\ref{fig:learning_order} (a), for example, all the dogs in one
category at the general level can be accurately inferred to be small or large. This is because all
the dogs in one cluster fit into the same category at the general level, and all the dogs in one
cluster are of same size.

When knowledge is represented as in Figure~\ref{fig:learning_order} (b), however, it becomes less
clear whether all the dogs in one category at the general level are of same size. To infer this, the
category of a dog has to be inferred at the general level first, and then, dogs from the same
category at the general level have to be searched through all the clusters. This extra steps of
inference and search may introduce error in inference. A large dog, for example, may be mistakenly
inferred to fit into the same category with a small dog at the general level. Then, all the dogs in
one category cannot be accurately inferred to be of same size. As a result, an inference may appear
as if the inclusion relation is neglected.

These influences of learning order make a contrast to the exemplar representation.  With the exemplar
representation, each object is assigned to its own cluster, regardless of learning order. Thus, the
exemplar representation does not explain the influences of learning order. We demonstrate this
contrast between the cluster and exemplar representations in the next section.


\subsection*{Learning order and knowledge representation}

\begin{figure}
    \centering

    \subfloat[Cluster representation. An inference tends to be more accurate at the level learned at
    first.]
    {%
        \includegraphics[]{./figure/order_accuracy_cluster.pdf}
    }

    \subfloat[Exemplar representation. An inference tends to be more accurate when the specific
    level is learned at first, but the effect appears to be minor. The difference is less than
    $.01$ at both levels.]
    {%
        \includegraphics[]{./figure/order_accuracy_exemplar.pdf}
    }

    \vspace{10pt}

    \caption{The average inference accuracy for each learning order.  The levels of categories are
    shown in columns: the left panels illustrate the general level, and the right panels illustrate
    the specific level.   A dot represents mean, and error bar is the empirical interval.}

\label{fig:result2}
\end{figure}

Here, we tested the hierarchical structures with two possible learning orders: general-first and
specific-first. A training block in this simulation presents the eight objects in a random order
with a category label at either general or specific level. Then, the learning order of general-first
involves 10 training blocks with the general level first, followed by 10 training blocks with the
specific level. This order is reversed for the learning order of specific-first.

Figure~\ref{fig:result2} illustrates the average inference accuracy for each learning order. This
figure shows that with the cluster representation, an inference tends to be more accurate at the
level learned at first.  The left panel in Figure~\ref{fig:result2} (a), for example, shows that
when the categories at the general level are learned at first, an inference is more accurate at the
general level than when the same categories are learned later. With the exemplar representation, in
contrast, the inference accuracy does not appear much influenced by the learning order.

These results show that with the cluster representation, learning is built upon the existing
representation. The cluster representation tends to mirror the category structure learned at first,
and this cluster structure carries over to the subsequent learning. As a result, an inference tends
to be more accurate at the level learned first.


\subsection*{Conclusion}

Although we do not argue against the possibility that knowledge is hierarchically represented, we
have demonstrated that how people categorize objects may not necessarily correspond to the way
knowledge is represented. In particular, we showed that the preference for the hierarchical
structure is not necessarily due to the hierarchical representation. Even with the cluster
representation, which does not distinguish general and specific levels, the hierarchical structure
is optimal and should be preferred.  The cluster representation also explains two major empirical
findings in relation to the hierarchical structure of categories. Here, we have shown that the
category structure learned at first shapes knowledge representation and an inference accuracy later
in life.


%% Following discussion on machine teaching is dropped, to give this paper more focus
% In addition, the simulation method we used complements machine teaching \parencite{Zhu2013a}.
% Machine teaching identifies the optimal training set, such that the learner trained on the set
% achieves the best performance on the testing set \parencite{Zhu2013a, Patil2014a}.  Thus in the
% machine teaching, the procedure is conditioned on the learning model and the category structure.
% Here in contrast, we searched through possible structures to identify the optimal structure, such
% that the learner achieves the best performance.


\printbibliography{}

%TC:ignore
\appendix

\section{Details of simulation}
\label{appendix:model}

Here, we describe the model of category learning, which we used in the simulation. The model was
proposed by \textcite{Anderson1991a} and subsequently extended by \textcite{Sanborn2010a}. Here, The
exemplar and cluster representations are simulated with the same model but with different parameter
values.

Suppose a learner has observed $n - 1$ objects $\left\{x_{1}, x_{2}, \dots, x_{n - 1}\right\}$ with
corresponding category labels $\left\{y_{1}, y_{2}, \dots, y_{n - 1}\right\}$.  Each of these
objects fits into a cluster. The cluster label for the $i$th object is denoted as $z_{i}$.

\subsection{Drawing an inference}

Then, the probability that the $n$th object fits into category $w$ is expressed as follows:
\begin{align}
    p(y_{n} = w \; \vert \; x_{n})
    &= \sum_{k \in \mathbb{Z}} p(z_{n} = k \; \vert \; x_{n}) \; p(y_{n} = w \; \vert \; z_{n} = k) \notag\\
    &= \sum_{k \in \mathbb{Z}} \; \frac{p(z_{n} = k) \; p(x_{n} \;\vert\; z_{n} = k)}{p(x_{n})} \; p(y_{n} = w \; \vert \; z_{n} =
    k) \notag\\
    &= \sum_{k \in \mathbb{Z}} \; \frac{p(z_{n} = k) \; p(x_{n} \;\vert\; z_{n} = k)}{\sum_{s \in
    \mathbb{Z}} \; p(z_{n} = s) \; p(x_{n} \;\vert\; z_{n} = s)} \; p(y_{n} = w \; \vert \; z_{n} = k).
\label{eqn:inference}
\end{align}
Here, $\mathbb{Z}$ is a set of all the possible clusters to which the $n$th object can be assigned.
The three terms in Equation~\ref{eqn:inference} are described below in turn.

First, the probability that the $n$th object fits into cluster $k$ is given by:
\begin{align}
    p(z_{n} = k) = \left\{
        \begin{array}{rcl}
            \displaystyle \frac{c\,m_{k}}{(1 - c) + c\,(n - 1)} & \mbox{if} & m_{k} > 0\\
            \\
            \displaystyle \frac{(1 - c)}{(1 - c) + c\,(n - 1)} & \mbox{if} & m_{k} = 0
        \end{array}
    \right.
\label{eqn:prior}
\end{align}
where $c$ is a parameter called the coupling probability and $m_{k}$ is the number of objects
already assigned to cluster $k$.

Following \textcite{Anderson1991a} and \textcite{Sanborn2010a}, we assume that an object has
independent dimensions given cluster. Therefore,
\begin{align}
    p(x_{n} \; \vert \; z_{n} = k) = \prod_{d \in D} p(x_{n,d} \; \vert \; z_{n} = k),
\label{eqn:feature1}
\end{align}
where $D$ is a set of dimensions in which an object is described.  The above term is computed with
\begin{align}
    p(x_{n,d} = v \; \vert \; z = k) = \frac{B_{v,d} + \beta_{c}}{m_{k} + J_{d} \, \beta_{c}},
\label{eqn:feature2}
\end{align}
where $B_{v,d}$ is the number of objects in cluster $k$ with value of $v$ on dimension $d$, and
$J_{d}$ is the number of values which an object can take on dimension $d$. Sensitivity parameter
$\beta_{c}$ determines knowledge representation, as discussed below.

Similarly, the probability that the $n$th object has category label $w$, given a cluster, is given by:
\begin{align}
    p(y_{n} = w \; \vert \; z = k) = \frac{B_{w} + \beta_{l}}{B_{.} + J \, \beta_{l}},
\label{eqn:label}
\end{align}
where $B_{w}$ is the number of observed objects with category label $w$ in cluster $k$, $B_{.}$ is
the number of object in cluster $k$, and $J$ is the number of category labels. Also, sensitivity
parameter $\beta_{l}$ determines knowledge representation, as discussed below.


\subsection{Learning}

The learning is equivalent to assigning an object into a cluster. The probability that an object is
assigned to cluster $k$ is computed as
\begin{align}
    p(z_{n} = k \;\vert\; x_{n},\, y_{n}) \propto
    p(z_{n} = k)
    \;
    p(x_{n} \;\vert\; z_{n} = k)
    \;
    p(y_{n} \;\vert\; z_{n} = k).
\label{eqn:learning}
\end{align}
This is computed with Equations~\ref{eqn:prior},~\ref{eqn:feature1} and~\ref{eqn:label}.
Additionally, this cluster assignment is conducted using the sequential Monte Carlo with one
particle, which produces behavior much like human \parencite{Sanborn2010a}.


\subsection{Knowledge representation}

With the above specification, knowledge representation is determined by values for sensitivity
parameter $\beta$.  For the cluster representation, we used $\beta_{f}=1.0$ and $\beta_{l}=0.5$.
These values are among the best fitting values to human performance \parencite{Sanborn2010a}.

For the exemplar representation, we used small values of $\beta$ ($\beta_{f}=0.001$ and
$\beta_{l}=0.001$) during the training, ensuring that a cluster can only contain identical objects.
Inference based on such small $\beta$ is, however, often deterministic, and to allow more
probabilistic inference, we used a larger value for $\beta$ ($\beta_{f}=1.0$ and $\beta_{l}=5.0$)
during the testing.

For both representations, the coupling probability $c$ is at $0.5$ during the training blocks and at
$1.0$ during the testing blocks. These coupling probabilities prevent a new cluster from being
created during the testing and ensure that an inference is not made with random guessing.

% Since the ordering of features is exchangeable, the
% number of possible category structure totals to 3,675.

%  In a
% trial with one level (e.g., the general level), a category label at the other level (e.g., the
% specific level) is treated as missing.

%TC:endignore
\end{document}
